{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E60IivOy8nUE"
      },
      "source": [
        "# Aula 5.1: Clustering <a id='home'></a>\n",
        "\n",
        "arquivos necessários = `iris.csv`\n",
        "\n",
        "Nesta aula, iremos examinar o*problema de clustering*. No problema de agrupamento, não sabemos quantas classes existem! Em vez disso, tentamos agrupar as observações em clusters com base em quão semelhantes são as observações.\n",
        "\n",
        "O agrupamento é útil para segmentar mercados consumidores (tanto em características quanto em geografia) para discriminar preços, por exemplo. Outros acham o agrupamento útil para identificar bots no twitter, encontrar blocos de eleitores que podem estar em cima do muro, combinar pessoas em sites de namoro ou agrupar calouros em dormitórios com base em preferências.\n",
        "\n",
        "Começaremos nossa discussão usando o algoritmo \"K-Means\" para identificar clusters nos dados, por ser simples e intuitivo. No final, expandiremos nossa análise para diferentes ideias de agrupamento. A agenda é a seguinte:\n",
        "\n",
        "1. [Por que clusters? Um exemplo](#dgp)\n",
        "2. [Escolhendo o número de clusters: o gráfico do cotovelo](#elbow)\n",
        "3. [Outros métodos de agrupamento](#others)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wYbzMWm8nUI"
      },
      "source": [
        "# 1. Por que Clusters? Um exemplo ([top](#home))<a id=\"dgp\"></a>\n",
        "\n",
        "O clustering é útil porque nos permite reduzir a dimensionalidade de grandes conjuntos de dados; podemos tornar o Big Data potencialmente pequeno. Começaremos com nosso conjunto de dados de íris porque sabemos que os dados estão realmente agrupados em torno das espécies. E se alguém nos fornecesse dados semelhantes que não incluíssem a variável `species`. Ainda poderíamos identificar esses agrupamentos?\n",
        "\n",
        "Podemos agrupar muitas características, mas se usarmos apenas duas, podemos visualizá-las em um gráfico de dispersão. Vamos usar as medidas das pétalas.\n",
        "\n",
        "Vamos fingir que não conhecemos as espécies de cada observação. Usaremos as informações das espécies mais tarde para ver como o método de agrupamento funcionou. Observe que nossa abordagem aqui é semelhante a quando introduzimos a econometria (OLS) e começamos gerando o verdadeiro processo de geração de dados (DGP). Essa abordagem nos permitiu conhecer \"a verdade\" para que pudéssemos avaliar o quão bem nossa abordagem funcionava sem erros de especificação. Mesma ideia aqui.\n",
        "\n",
        "Vamos carregar os dados e dar uma olhada para nos lembrarmos do que temos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-RctyJn8nUJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sklearn.cluster\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "iris = pd.read_csv('iris.csv')\n",
        "iris.head(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwuUhlZB8nUL"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10,6))\n",
        "\n",
        "ax.scatter(iris['petal length'], iris['petal width'], color='blue')\n",
        "\n",
        "ax.set_xlabel('petal length (cm)')\n",
        "ax.set_ylabel('petal width (cm)')\n",
        "\n",
        "sns.despine()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wck81Q9Q8nUM"
      },
      "source": [
        "Todos os pontos estão da mesma cor de propósito. Estou fingindo que não sei a qual espécie cada ponto pertence. Na verdade, estou fingindo que nem sei quantas espécies existem.\n",
        "\n",
        "Eu vejo duas bolhas distintas. Defina $k$ como o número de clusters que observamos nos dados. A partir da imagem, parece razoável supor que existam dois grupos (ou seja, $k=2$) nestes dados, mas é claro que sabemos que existem três. Vamos ajustar o modelo com base em nosso palpite.\n",
        "\n",
        "Como vimos no problema do classificador, precisamos de arrays numpy para trabalhar com sklearn. Abaixo, converto as duas características em matrizes numpy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvQnqbmm8nUN"
      },
      "outputs": [],
      "source": [
        "X = iris[['petal width', 'petal length']].to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcHQOLUo8nUO"
      },
      "source": [
        "## Agrupamento K-means\n",
        "\n",
        "O K-means gera agrupamentos procurando por centros de círculos de dados. O algoritmo varia a localização dos centróides dos clusters até minimizar a distância média do centro dos clusters aos seus membros. A receita é, portanto:\n",
        "\n",
        "1. Selecione um número de classes/grupos para usar e inicialize aleatoriamente seus respectivos pontos centrais. Para descobrir o número de classes a serem usadas, é bom dar uma olhada rápida nos dados e tentar identificar quaisquer agrupamentos distintos (ou seja, o que fizemos acima).\n",
        "\n",
        "\n",
        "2. Cada ponto de dados é classificado pelo cálculo da distância entre esse ponto e cada centro do grupo. Cada ponto é então classificado como o ponto que está no grupo cujo centro está mais próximo dele.\n",
        "\n",
        "\n",
        "3. Em seguida, recalculamos o centro do grupo tomando a média de todos os vetores do grupo.\n",
        "\n",
        "\n",
        "4. Repita essas etapas para um número definido de iterações ou até que os centros do grupo não mudem muito entre as iterações. Você também pode optar por inicializar aleatoriamente os centros de grupo algumas vezes e, em seguida, selecionar a execução que parece fornecer os melhores resultados.\n",
        "\n",
        "\n",
        "O K-Means tem a vantagem de ser *rápido* (e intuitivo), pois estamos apenas calculando as distâncias entre pontos e centros de grupos. Claro, não existe almoço grátis e veremos que kmeans tem desvantagens. Há também um algoritmo K-Medians que é menos sensível a valores discrepantes, mas também mais lento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOLTFabZ8nUP"
      },
      "outputs": [],
      "source": [
        "# The random_state option ensures we all get the same answer.\n",
        "kmeans = sklearn.cluster.KMeans(n_clusters=2, random_state=0).fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCu25vHW8nUQ"
      },
      "source": [
        "O objeto retornado do modelo contém (entre outras coisas) as coordenadas dos centros. Eles são matrizes numpy. Vou convertê-los em um dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LlGZfC_8nUU"
      },
      "outputs": [],
      "source": [
        "centers = pd.DataFrame(kmeans.cluster_centers_, columns=['y', 'x'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nvUoDFi8nUW"
      },
      "source": [
        "E vamos representar graficamente os resultados onde os dados brutos denotarei como pontos azuis como antes e estou colocando uma estrela nos centros encontrados acima."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQDncQBb8nUX"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10,6))\n",
        "\n",
        "ax.scatter(iris['petal length'], iris['petal width'], color='blue')\n",
        "ax.scatter(centers['x'], centers['y'], color='red', s=200, marker='o')\n",
        "\n",
        "ax.set_xlabel('petal length (cm)')\n",
        "ax.set_ylabel('petal width (cm)')\n",
        "\n",
        "sns.despine()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no2C7gm88nUX"
      },
      "source": [
        "Isso é sobre o que eu esperava de 'olhômetro'.\n",
        "\n",
        "Sabemos que existem três tipos de íris no conjunto de dados. Vamos tentar 3 clusters e representar graficamente os resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOdsYbrI8nUY"
      },
      "outputs": [],
      "source": [
        "kmeans = sklearn.cluster.KMeans(n_clusters=3, random_state=0).fit(X)\n",
        "centers = pd.DataFrame(kmeans.cluster_centers_, columns=['y', 'x'])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,6))\n",
        "\n",
        "ax.scatter(iris['petal length'], iris['petal width'], color='blue')\n",
        "ax.scatter(centers['x'], centers['y'], color='red', s=200, marker='o')\n",
        "\n",
        "ax.set_xlabel('petal length (cm)')\n",
        "ax.set_ylabel('petal width (cm)')\n",
        "\n",
        "sns.despine()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Jo3fqdy8nUY"
      },
      "source": [
        "Esses centródies fazem sentido? Vamos recuperar os `labels` conhecidos para cada um dos pontos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7j6ry2Dr8nUZ"
      },
      "outputs": [],
      "source": [
        "pred = pd.DataFrame(kmeans.labels_, columns=['predicted'])\n",
        "iris = pd.merge(left=pred, right=iris, left_index=True, right_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eum0fkw68nUZ"
      },
      "source": [
        "Vamos dar uma olhada para ver se os dados foram juntados corretamente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLPRsV6B8nUn"
      },
      "outputs": [],
      "source": [
        "iris.sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1z_mm2u8nUo"
      },
      "source": [
        "Então o que aconteceu aqui? O algoritmo de agrupamento identificou três agrupamentos de dados e os rotulou como 0, 1 e 2. Sabemos que os dados são agrupados por espécies, então vamos dar uma olhada em como as categorias se alinham com as espécies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZueC-5f8nUp"
      },
      "outputs": [],
      "source": [
        "iris.groupby('species')['predicted'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1qccb-s8nUq"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1,2,figsize=(15,6))\n",
        "\n",
        "sns.scatterplot(ax=ax[0], x='petal length', y='petal width', hue='predicted', data=iris,palette=['#377eb8', '#4daf4a', '#ff7f00'])\n",
        "\n",
        "sns.scatterplot(ax=ax[1], x='petal length', y='petal width', hue='species', data=iris)\n",
        "\n",
        "ax[0].scatter(centers['x'], centers['y'], color='red', s=200, marker='o')\n",
        "\n",
        "ax[0].set_title('Clusters Preditos',size=20)\n",
        "ax[1].set_title('Clusters Verdadeiros',size=20)\n",
        "\n",
        "ax[0].legend(frameon=False)\n",
        "ax[1].legend(frameon=False)\n",
        "\n",
        "sns.despine(ax=ax[0])\n",
        "sns.despine(ax=ax[1])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XUqfqEq8nUr"
      },
      "source": [
        "Observamos nos dados groupby e nas imagens que o algoritmo faz um bom trabalho. O cluster setosa é perfeito &mdash; o que não é surpreendente. A divisão entre os outros dois tipos é decente, mas não perfeita, pois esses dados se sobrepõem mais."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEeglCJC8nUr"
      },
      "source": [
        "# 2. Escolhendo o número de clusters: o gráfico de cotovelo. ([top](#home))<a id=\"elbow\"></a>\n",
        "\n",
        "Quando olhei para os dados e fingi que não sabia quantas espécies havia, pensei que eram dois aglomerados. Quando adicionei um terceiro, ele fez um bom trabalho combinando os dados.\n",
        "\n",
        "Mas o ponto principal desses modelos é que **não sabemos quantos clusters existem.** Então, como escolhemos o número de clusters?\n",
        "\n",
        "A resposta curta: Tente um monte para ver o que acontece.\n",
        "\n",
        "Podemos fazer um loop sobre o número de clusters e acompanhar a soma da distância quadrada entre os centros dos clusters e os membros dos clusters. Se houver lacunas nos dados (ou seja, clusters), adicionar um cluster adicional reduzirá drasticamente essas distâncias. A soma da medida da distância ao quadrado é capturada no método `inertia`. Vamos ver como isso funciona:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hQQ_DYJ8nUr"
      },
      "outputs": [],
      "source": [
        "y=[]  # Start with an empty df\n",
        "for n in range(1,8):  # loop from 1 to 7 clusters\n",
        "    kmeans = sklearn.cluster.KMeans(n_clusters=n, random_state=0).fit(X)\n",
        "    y.append(kmeans.inertia_)\n",
        "    \n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DAClGpI8nUs"
      },
      "source": [
        "Parece que esses números mudam muito. Vamos dar uma olhada nas medidas em uma imagem:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knLirLno8nUs"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(15,6))\n",
        "\n",
        "ax.plot(range(1,8), y, color='blue', marker='.')\n",
        "\n",
        "ax.set_xlabel('number of clusters (k)')\n",
        "\n",
        "ax.set_ylabel('sum of squared distances to nearest cluster')\n",
        "\n",
        "sns.despine(ax=ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVxm63kz8nUs"
      },
      "source": [
        "Vemos que a mudança de 1 para 2 clusters fez uma grande melhoria, como uma espécie de \"cotovelo\" do gráfico porque a figura parece um braço, eu acho. Passar de 2 para 3 clusters não melhorou tanto assim.\n",
        "\n",
        "Sempre podemos diminuir a soma da distância ao quadrado aumentando k. Quando k é igual ao número de observações, então cada cluster tem exatamente uma observação e a soma dos quadrados é zero. Mas isso não é muito útil: diríamos que há retornos decrescentes para k.\n",
        "\n",
        "A partir desta figura, eu escolheria k=2 ou k=3. Depois disso, não obtemos muita melhoria nas somas dos quadrados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1OPF6Ox8nUs"
      },
      "source": [
        "# Atividade 1\n",
        "\n",
        "Vamos tentar responder à seguinte pergunta: quais cidades do Ceará são semelhantes a cidades do estado de Pernambuco? Talvez sua empresa tenha gasto muito dinheiro pesquisando o Ceará como um mercado. Agora quer se expandir para Pernambuco. Podemos encontrar cidades em Pernambuco que sejam semelhantes às do Ceará? Vamos usar nosso modelo de cluster para ver.\n",
        "\n",
        "1. Vamos carregar uma base de dados da [API do IBGE](https://servicodados.ibge.gov.br/api/docs/)\n",
        "**OBS**: pode ser necessário gerar uma query [neste link](https://servicodados.ibge.gov.br/api/docs/agregados?versao=3#api-bq)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests                    # api module\n",
        "import json\n",
        "\n",
        "url = 'https://servicodados.ibge.gov.br/api/v3/agregados/3974/periodos/2010/variaveis/3948?localidades=N6[N3[23,26]]&classificacao=12085[100543]|58[95253]'\n",
        "    \n",
        "response = requests.get(url)"
      ],
      "metadata": {
        "id": "gpp8WIQ7F_GW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos observar o valor da resposta obtida"
      ],
      "metadata": {
        "id": "e1UehknxZDv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resp = response.json()\n",
        "print(resp)"
      ],
      "metadata": {
        "id": "k2PPYwSvWYMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A resposta JSON está em níveis. Vamos precisar nos níveis acessando a chave 'resultados:"
      ],
      "metadata": {
        "id": "HfpIc_GPcqfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for item in resp:\n",
        "  for key in item['resultados']:\n",
        "    respJson = key\n",
        "respJson.pop('classificacoes')"
      ],
      "metadata": {
        "id": "jBn1vtT0YdhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos verificar o formato do novo arquivo JSON:"
      ],
      "metadata": {
        "id": "AJK419EwuoW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(respJson)"
      ],
      "metadata": {
        "id": "5x1GXHanuloM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nosso objeto JSON está bastante aninhado, ainda (nested). Vamos tentar utilizar o método [json_normalize](https://pandas.pydata.org/docs/reference/api/pandas.json_normalize.html) da biblioteca PANDAS:"
      ],
      "metadata": {
        "id": "W-Z0h25UvpJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pibPerCapita_CE_PE = pd.json_normalize(respJson['series'])\n",
        "pibPerCapita_CE_PE.head()"
      ],
      "metadata": {
        "id": "DCaWK9uhtCRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "UFA! Conseguimos obter um dataframe a partir dos dados da API do IBGE. Vamos fazer algumas modificações no dataframe:"
      ],
      "metadata": {
        "id": "XnyI9paHv_dU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pibPerCapita_CE_PE.drop(columns=[pibPerCapita_CE_PE.columns[1],pibPerCapita_CE_PE.columns[2]],inplace=True)\n",
        "pibPerCapita_CE_PE.head()"
      ],
      "metadata": {
        "id": "PNheKBgSwlu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pibPerCapita_CE_PE.rename(columns = {pibPerCapita_CE_PE.columns[0]:'ID', \n",
        "                                     pibPerCapita_CE_PE.columns[1]:'CIDADE', \n",
        "                                     pibPerCapita_CE_PE.columns[2]: 'PIB'}, inplace = True)\n",
        "pibPerCapita_CE_PE.set_index('ID', inplace = True)"
      ],
      "metadata": {
        "id": "7tt43eFHwjHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ainda podemos separar a localidade em UF + Cidade, vamos fazer:"
      ],
      "metadata": {
        "id": "S6Pk1TL0zMKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pibPerCapita_CE_PE[['CIDADE','UF']] = pibPerCapita_CE_PE['CIDADE'].str.split(' - ',1).tolist()\n",
        "pibPerCapita_CE_PE.head()"
      ],
      "metadata": {
        "id": "9d-4kmJVzHGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_columns = (pibPerCapita_CE_PE.columns.drop('PIB').tolist()) + ['PIB']\n",
        "pibPerCapita_CE_PE = pibPerCapita_CE_PE[new_columns]\n",
        "pibPerCapita_CE_PE.head()"
      ],
      "metadata": {
        "id": "-aDmpCdh6hZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ótimo, temos um dataframe organizado para clusterização. Porém, o PIB per capita sozinho não é útil para nosso problema de clusterização. Precisamos encontrar outra variável: vamos adotar a densidade demográfica:"
      ],
      "metadata": {
        "id": "ksm1GyMm1FuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://servicodados.ibge.gov.br/api/v3/agregados/1301/periodos/2010/variaveis/616?localidades=N6[N3[23,26]]\"\n",
        "response = requests.get(url)"
      ],
      "metadata": {
        "id": "llEltMDJ1GNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resp = response.json()\n",
        "for item in resp:\n",
        "  for key in item['resultados']:\n",
        "    respJson = key\n",
        "respJson.pop('classificacoes')\n",
        "\n",
        "densidade_CE_PE = pd.json_normalize(respJson['series'])\n",
        "densidade_CE_PE.head()"
      ],
      "metadata": {
        "id": "I-l9v7Ks2DlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "densidade_CE_PE.drop(columns=[densidade_CE_PE.columns[1],densidade_CE_PE.columns[2],densidade_CE_PE.columns[3]],inplace=True)\n",
        "densidade_CE_PE.rename(columns = {densidade_CE_PE.columns[0]:'ID', \n",
        "                                     densidade_CE_PE.columns[1]: 'DENS'}, inplace = True)\n",
        "densidade_CE_PE.set_index('ID', inplace = True)\n",
        "densidade_CE_PE.head()"
      ],
      "metadata": {
        "id": "x8dXpOzD52RK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora podemos juntar os dataframes por ID:"
      ],
      "metadata": {
        "id": "XzxVZ4Zu6pr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_CE_PE = pd.merge(pibPerCapita_CE_PE, densidade_CE_PE, on=\"ID\")\n",
        "cluster_CE_PE"
      ],
      "metadata": {
        "id": "e2l4iHUm6ay6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrvTkpkn8nUu"
      },
      "source": [
        "Vamos fazer renda per capita e densidade. Você vê um relacionamento? Um modelo linear seria útil aqui?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeW6RN7W8nUu"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "fig, ax = plt.subplots(figsize=(15,7))\n",
        "\n",
        "ax.scatter(cluster_CE_PE['DENS'], cluster_CE_PE['PIB'], color='blue')\n",
        "\n",
        "ax.set_xlabel('Densidade (hab/km2)')\n",
        "ax.set_ylabel('PIB per capita (R$)')\n",
        "sns.despine(ax=ax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqUdvtWF8nUv"
      },
      "source": [
        "Vamos o gráfico do cotovelo usando `DENS` e `PIB` como variáveis. Tente de 1 a 10 clusters possíveis. Lembre-se, você precisa converter seus dados DataFrame em matrizes numpy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgoN_WBq8nUv"
      },
      "outputs": [],
      "source": [
        "import sklearn.cluster\n",
        "# Convert to numpy\n",
        "X = cluster_CE_PE[['DENS', 'PIB']].to_numpy()\n",
        "\n",
        "# Try the model with 1, 2, ..., 10 clusters. Keep track of the ssd ('inertia')\n",
        "y=[]\n",
        "for n in range(1, 11):\n",
        "    kmeans = sklearn.cluster.KMeans(n_clusters=n, random_state=0).fit(X)\n",
        "    y.append(kmeans.inertia_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhDWqHmz8nUw"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(15,6))\n",
        "\n",
        "ax.plot(range(1,11), y, color='blue', marker='.')\n",
        "\n",
        "ax.set_xlabel('number of clusters (k)')\n",
        "\n",
        "ax.set_ylabel('sum of squared distances to nearest cluster')\n",
        "\n",
        "sns.despine(ax=ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CK91ieUu8nUw"
      },
      "source": [
        "Com base em seu gráfico de cotovelo, quantos clusters parecem apropriados?\n",
        "\n",
        "**Resposta:** Parece que 3 clusters nos dão um bom insight inicial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwLjCpvU8nUw"
      },
      "source": [
        "Vamos explorar nossos resultados um pouco mais. Vamos executar o modelo kmeans com k=3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcH3KIn98nUw"
      },
      "outputs": [],
      "source": [
        "kmeans = sklearn.cluster.KMeans(n_clusters=3, random_state=0).fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zSjS4UQ8nUx"
      },
      "source": [
        "Recupere os `labels_` dos resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SzFgmPA8nUx"
      },
      "outputs": [],
      "source": [
        "clusters = pd.DataFrame(kmeans.labels_, columns=['cluster'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CE0QxiJ58nUx"
      },
      "source": [
        "Mescle os rótulos em seu DataFrame original, usando o índice como chave de mesclagem."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_CE_PE"
      ],
      "metadata": {
        "id": "Uk4f5AOvRTuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XooIZFe_8nUx"
      },
      "outputs": [],
      "source": [
        "detail = pd.merge(left=cluster_CE_PE.reset_index(), right=clusters, left_index=True, right_index=True)\n",
        "detail[['PIB','DENS']]=detail[['PIB','DENS']].astype(float)\n",
        "detail.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RR_k6tax8nUx"
      },
      "source": [
        "Vamos plotar `PIB` e `DENS` novamente usando o gráfico de dispersão do seaborn. Defina o matiz para os `labels_` que você recuperou. que padrões você vê? Você pode adicionar rótulos aos pontos para saber quais municípios são quais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roqlUAFN8nUx"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(15,6))\n",
        "\n",
        "sns.scatterplot(ax=ax, x='DENS', y='PIB', hue='cluster', data=detail, palette=['#377eb8', '#ff7f00', '#4daf4a'])\n",
        "\n",
        "for x,y,t in zip(detail['DENS'], detail['PIB'], detail['CIDADE']):\n",
        "    if y > 500:\n",
        "        ax.text(x+10, y+1, t)\n",
        "\n",
        "ax.set_xlabel('Densidade (hab/km2)')\n",
        "ax.set_ylabel('PIB per Capita')\n",
        "\n",
        "sns.despine(ax=ax)\n",
        "ax.legend(frameon=False)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHz3qW6s8nUy"
      },
      "source": [
        "Vamos refazer com k=4. Como os resultados mudam?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lb0-wwPy8nUy"
      },
      "outputs": [],
      "source": [
        "kmeans = sklearn.cluster.KMeans(n_clusters=4, random_state=0).fit(X)\n",
        "clusters = pd.DataFrame(kmeans.labels_, columns=['cluster'])\n",
        "detail = pd.merge(left=cluster_CE_PE.reset_index(), right=clusters, left_index=True, right_index=True)\n",
        "detail[['PIB','DENS']]=detail[['PIB','DENS']].astype(float)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15,6))\n",
        "\n",
        "sns.scatterplot(ax=ax, x='DENS', y='PIB', hue='cluster', data=detail)\n",
        "\n",
        "for x,y,t in zip(detail['DENS'], detail['PIB'], detail['CIDADE']):\n",
        "    if y > 500:\n",
        "        ax.text(x+10, y+1, t)\n",
        "\n",
        "ax.set_xlabel('Densidade (hab/km2)')\n",
        "ax.set_ylabel('PIB per Capita')\n",
        "\n",
        "sns.despine(ax=ax)\n",
        "ax.legend(frameon=False)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "E se aumentarmos para 8 clusters?"
      ],
      "metadata": {
        "id": "4Pyz7Iw09_PN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = sklearn.cluster.KMeans(n_clusters=4, random_state=0).fit(X)\n",
        "clusters = pd.DataFrame(kmeans.labels_, columns=['cluster'])\n",
        "detail = pd.merge(left=cluster_CE_PE.reset_index(), right=clusters, left_index=True, right_index=True)\n",
        "detail[['PIB','DENS']]=detail[['PIB','DENS']].astype(float)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15,6))\n",
        "ax.set(xscale=\"log\", yscale=\"log\")\n",
        "\n",
        "sns.scatterplot(ax=ax, x='DENS', y='PIB', hue='cluster', data=detail)\n",
        "\n",
        "for x,y,t in zip(detail['DENS'], detail['PIB'], detail['CIDADE']):\n",
        "    if y > 450:\n",
        "        ax.text(x-10, y-5, t)\n",
        "\n",
        "ax.set_xlabel('Densidade (hab/km2)')\n",
        "ax.set_ylabel('PIB per Capita')\n",
        "#ax.set_xlim(10, 500)\n",
        "#ax.set_ylim(300, 1100)\n",
        "sns.despine(ax=ax)\n",
        "ax.legend(frameon=False)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9WQYW2Gk9_er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4NQMlvg8nUy"
      },
      "source": [
        "## Comentários\n",
        "\n",
        "O K-Means tem algumas desvantagens.\n",
        "\n",
        "1. Você deve selecionar quantos grupos existem. A abordagem do Gráfico de cotovelo ajuda, mas não há teste estatístico para nos apoiarmos - ou pelo menos identificar uma prática recomendada entre diferentes pesquisadores. Isso nem sempre é trivial e, idealmente, com um algoritmo de cluster, gostaríamos que descobrisse isso para nós, porque o objetivo é obter alguns insights dos dados.\n",
        "\n",
        "\n",
        "2. K-means começa com uma escolha aleatória de centros de cluster e, portanto, pode produzir diferentes resultados de cluster em diferentes execuções do algoritmo. Assim, os resultados podem não ser repetíveis e não ter consistência. Outros métodos de cluster são mais consistentes.\n",
        "\n",
        "\n",
        "3. O K-means desenha fundamentalmente círculos ao redor dos dados. E se os círculos não forem a melhor maneira de agrupar os dados. Observe os gráficos de dispersão no exercício prático. Os dados de densidade próxima de zero parecem uma coluna (ou seja, muita heterogeneidade na renda per capita), enquanto parece haver uma relação positiva entre densidade e renda per capita entre densidade de 0 e 200. Parece que esses dois grupos deveriam ser clusters diferentes, mas o K-Means terá dificuldade em fazer isso, pois procura por centros de círculos e, portanto, sempre escolherá um centro no meio de ambos os grupos para capturar ambos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6aE2AQJ8nU0"
      },
      "source": [
        "# 3. Outros métodos de agrupamento ([top](#home))<a id=\"others\"></a>\n",
        "\n",
        "* **Agrupamento de deslocamento médio (MeanShift).** O agrupamento de deslocamento médio é um algoritmo baseado em janela deslizante que tenta encontrar áreas densas de pontos de dados. É um algoritmo baseado em centroides, o que significa que o objetivo é localizar os pontos centrais de cada grupo/classe, que funciona atualizando candidatos para pontos centrais para serem a média dos pontos dentro da janela deslizante. Essas janelas candidatas são então filtradas em um estágio de pós-processamento para eliminar quase duplicatas, formando o conjunto final de pontos centrais e seus grupos correspondentes.\n",
        "\n",
        "     Em contraste com o clustering K-means, não há necessidade de selecionar o número de clusters, pois o desvio médio descobre isso automaticamente - essa é uma boa vantagem. O fato de os centros dos clusters convergirem para os pontos de densidade máxima também é bastante desejável, pois é bastante intuitivo de entender e se encaixa bem em um sentido naturalmente orientado a dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jbUx7Nb8nU1"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
        "\n",
        "# The following bandwidth can be automatically detected using\n",
        "bandwidth = estimate_bandwidth(X, quantile=0.2, n_samples=500)\n",
        "\n",
        "ms = MeanShift(bandwidth=bandwidth, bin_seeding=True).fit(X)\n",
        "clusters = pd.DataFrame(ms.labels_, columns=['cluster'])\n",
        "detail = pd.merge(left=cluster_CE_PE.reset_index(), right=clusters, left_index=True, right_index=True)\n",
        "detail[['PIB','DENS']]=detail[['PIB','DENS']].astype(float)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15,6))\n",
        "ax.set(xscale=\"log\", yscale=\"log\")\n",
        "\n",
        "sns.scatterplot(ax=ax, x='DENS', y='PIB', hue='cluster', data=detail)\n",
        "\n",
        "for x,y,t in zip(detail['DENS'], detail['PIB'], detail['CIDADE']):\n",
        "    if y > 450:\n",
        "        ax.text(x-10, y-1, t)\n",
        "\n",
        "ax.set_xlabel('Densidade (hab/km2)')\n",
        "ax.set_ylabel('PIB per Capita')\n",
        "#ax.set_xlim(10, 500)\n",
        "#ax.set_ylim(300, 1100)\n",
        "sns.despine(ax=ax)\n",
        "ax.legend(frameon=False)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsTaKH448nU1"
      },
      "source": [
        "O algoritmo selecionou 12 clusters. Essa foi uma boa opção de agrupamento para nossos dados? Eu argumentaria \"não\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv3QDSer8nU2"
      },
      "source": [
        "* **Agrupamento espacial de aplicativos com ruído baseado em densidade (DBSCAN).** DBSCAN é uma forma de algoritmo de agrupamento de **vizinho mais próximo** que você normalmente ouve sendo aplicado a dados. A ideia aproximada é que você trabalhe com os dados ao longo das dimensões dos dados e continue adicionando pontos de dados a um cluster até chegar a um conjunto de dados que seja suficientemente diferente. É um algoritmo clusterizado baseado em densidade semelhante ao desvio médio, mas com algumas vantagens notáveis. Primeiro, ele não requer um número pré-definido de clusters. O componente central do DBSCAN é o conceito de *core samples* -- amostras que estão em áreas de alta densidade. Um \"cluster\" é, portanto, um conjunto de *amostras de núcleo* que estão próximas umas das outras (medidas por alguma medida de distância). Qualquer amostra principal faz parte de um cluster, por definição. Qualquer amostra que não seja uma amostra principal e esteja pelo menos 'eps' de distância de qualquer amostra principal é considerada um valor discrepante. Em segundo lugar, identifica outliers como ruído (o que pode ser útil no nosso caso), enquanto k-means e mean-shift simplesmente os lançam em um cluster, mesmo que o ponto de dados seja muito diferente. Terceiro, ele pode encontrar clusters de tamanho e formato arbitrários muito bem.\n",
        "\n",
        "    **No lado negativo, pode ser difícil quando há muita variação na densidade do cluster.**\n",
        "    \n",
        "    Existem dois parâmetros para o algoritmo: `min_samples` e `eps`. Estes definem formalmente o que queremos dizer quando dizemos \"denso\".\n",
        "    \n",
        "    O parâmetro `min_samples` controla principalmente a tolerância do algoritmo em relação ao ruído (em conjuntos de dados ruidosos e grandes, pode ser desejável aumentar esse parâmetro), embora os resultados não sejam muito sensíveis a esse parâmetro, portanto, não é muito importante IMO. **O parâmetro `eps` é, no entanto, crucial para escolher adequadamente o conjunto de dados.** Ele controla a vizinhança local dos pontos. Quando escolhido muito pequeno, a maioria dos dados não será agrupada (e rotulada como `-1` para \"ruído\"). Quando escolhido muito grande, faz com que clusters próximos sejam mesclados em um cluster e, eventualmente, todo o conjunto de dados seja retornado como um único cluster. Algumas heurísticas para a escolha deste parâmetro têm sido discutidas na literatura, por exemplo, com base em um cotovelo no gráfico de distâncias do vizinho mais próximo. Vamos dar uma olhada nos nossos dados.\n",
        "    \n",
        "    ### Resolvendo para o 'eps' ideal: outro cotovelo\n",
        "\n",
        "    Podemos calcular a distância de cada ponto ao seu vizinho mais próximo usando o pacote `NearestNeighbors`. O ponto em si está incluído em `n_neighbors`. O método `kneighbors` retorna dois arrays, um que contém a distância para os pontos `n_neighbors` mais próximos e o outro que contém o índice para cada um desses pontos. Assim como em nosso gráfico de cotovelo anterior, estamos procurando um ponto de inflexão em que a mudança de `eps` realmente não tenha efeito sobre as distâncias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABiKHUQF8nU3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "neigh = NearestNeighbors(n_neighbors=2)\n",
        "nbrs = neigh.fit(X)\n",
        "distances, indices = nbrs.kneighbors(X)\n",
        "\n",
        "# Sort the results\n",
        "distances = np.sort(distances, axis=0)\n",
        "distances = distances[:,1]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15,6))\n",
        "\n",
        "plt.plot(distances, color='blue', marker='.')\n",
        "\n",
        "ax.set_xlabel('distance')\n",
        "\n",
        "ax.set_ylabel('eps')\n",
        "\n",
        "sns.despine(ax=ax)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofyH_66a8nU3"
      },
      "source": [
        "Parece muita curvatura quando `eps` é igual a 350. Vamos tentar isso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yT8CPb-p8nU3"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Compute DBSCAN with eps = 350\n",
        "db = DBSCAN(eps=350, min_samples=5).fit(X)\n",
        "\n",
        "clusters = pd.DataFrame(db.labels_, columns=['cluster'])\n",
        "detail = pd.merge(left=cluster_CE_PE.reset_index(), right=clusters, left_index=True, right_index=True)\n",
        "detail[['PIB','DENS']]=detail[['PIB','DENS']].astype(float)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15,6))\n",
        "ax.set(xscale=\"log\", yscale=\"log\")\n",
        "\n",
        "sns.scatterplot(ax=ax, x='DENS', y='PIB', hue='cluster', data=detail)\n",
        "\n",
        "for x,y,t in zip(detail['DENS'], detail['PIB'], detail['CIDADE']):\n",
        "    if y > 450:\n",
        "        ax.text(x-10, y-1, t)\n",
        "\n",
        "ax.set_xlabel('Densidade (hab/km2)')\n",
        "ax.set_ylabel('PIB per Capita')\n",
        "#ax.set_xlim(10, 500)\n",
        "#ax.set_ylim(300, 1100)\n",
        "sns.despine(ax=ax)\n",
        "ax.legend(frameon=False)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5P4j5Er8nU3"
      },
      "source": [
        "Hmmm... não é ótimo. Na verdade, brincar com o `eps` demonstra que estamos apenas mudando quais observações são ruído. Os agrupamentos que observamos com nossos globos oculares são muito heterogêneos para que esse método funcione bem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gobmPPKa8nU4"
      },
      "source": [
        "* **Agrupamento de Expectativa-Maximização (EM) usando Modelos de Mistura Gaussianas (GMM).** Uma das principais desvantagens do K-Means é seu uso ingênuo do valor médio para o centro do cluster e sua insistência em identificar clusters com círculos. A solução alternativa é relaxar a suposição do círculo e usar modelos de mistura gaussiana (GMMs) para nos dar mais flexibilidade. Com GMMs, assumimos que os pontos de dados são distribuídos em Gauss; esta é uma suposição menos restritiva do que dizer que eles são circulares usando a média. Dessa forma, temos dois parâmetros para descrever a forma dos clusters: a média e o desvio padrão! Tomando um exemplo em duas dimensões, isso significa que os aglomerados podem assumir qualquer tipo de forma elíptica (já que temos um desvio padrão nas direções x e y). Assim, cada distribuição gaussiana é atribuída a um único cluster. Assim, poderíamos usar essa abordagem de agrupamento para identificar os diferentes agrupamentos em nossos dados de prática. Para encontrar os parâmetros da Gaussiana para cada cluster (por exemplo, a média e o desvio padrão), usaremos um algoritmo de otimização chamado Expectativa-Maximização (EM).\n",
        "\n",
        "    Isso parece ser melhor para os nossos dados. Vamos tentar usando cinco clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y72Vq0LO8nU4"
      },
      "outputs": [],
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "gm = GaussianMixture(n_components=5, random_state=0).fit(X)\n",
        "clusters = pd.DataFrame(gm.predict(X), columns=['cluster'])\n",
        "detail = pd.merge(left=cluster_CE_PE.reset_index(), right=clusters, left_index=True, right_index=True)\n",
        "detail[['PIB','DENS']]=detail[['PIB','DENS']].astype(float)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15,6))\n",
        "ax.set(xscale=\"log\", yscale=\"log\")\n",
        "\n",
        "sns.scatterplot(ax=ax, x='DENS', y='PIB', hue='cluster', data=detail)\n",
        "\n",
        "for x,y,t in zip(detail['DENS'], detail['PIB'], detail['CIDADE']):\n",
        "    if y > 450:\n",
        "        ax.text(x-10, y-1, t)\n",
        "\n",
        "ax.set_xlabel('Densidade (hab/km2)')\n",
        "ax.set_ylabel('PIB per Capita')\n",
        "#ax.set_xlim(10, 500)\n",
        "#ax.set_ylim(300, 1100)\n",
        "sns.despine(ax=ax)\n",
        "ax.legend(frameon=False)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGEZyTgQ8nU4"
      },
      "source": [
        "* **Cluster hierárquico aglomerativo.** Os algoritmos de agrupamento hierárquico se dividem em 2 categorias: *top-down* ou *bottom-up*. Os algoritmos bottom-up tratam cada ponto de dados como um único cluster no início e, em seguida, mesclam sucessivamente (ou aglomeram) pares de clusters até que todos os clusters tenham sido mesclados em um único cluster que contém todos os pontos de dados. O agrupamento hierárquico de baixo para cima é, portanto, chamado agrupamento aglomerativo hierárquico (HAC). Essa hierarquia de clusters é representada como uma árvore (ou dendrograma). \n",
        "\n",
        "A raiz da árvore é o único cluster que reúne todas as amostras, sendo as folhas os clusters com apenas uma amostra. [Para saber mais sobre dendogramas](https://en.wikipedia.org/wiki/Dendrogram) é a página wiki.\n",
        "\n",
        "Observe que essa técnica pode ser particularmente útil para \"redução de recursos\" que é popular em finanças (onde esse tipo de análise é conhecido como \"Análise de Componentes Principais\"). Fundamentalmente, é uma ferramenta útil de redução de dimensionalidade que na linguagem de ML é conhecida como \"aprendizagem não supervisionada\".\n",
        "    \n",
        "Vamos tentar com nossos dados...\n",
        "    \n",
        "#### Escolhendo o número de clusters: o dendrograma\n",
        "Primeiro, temos que decidir quantos clusters designar. Como antes, podemos avaliar os dados diretamente para procurar lacunas. Em vez de um cotovelo, porém, podemos usar um dendrograma para visualizar o histórico de agrupamentos e descobrir o número ideal de agrupamentos. Identificamos o número ideal de clusters da seguinte forma:\n",
        "1. Determine a maior distância vertical que não cruza nenhum dos outros clusters.\n",
        "2. Desenhe uma linha horizontal entre as extremidades.\n",
        "3. O número ideal de clusters é igual ao número de linhas verticais que passam pela linha horizontal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAH311kn8nU4"
      },
      "outputs": [],
      "source": [
        "import scipy.cluster.hierarchy as sch # Used to create a dendrogram\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15,6))\n",
        "\n",
        "dendrogram = sch.dendrogram(sch.linkage(X, method='ward'),ax=ax,orientation='right')\n",
        "\n",
        "# Different methods to measure linkags / distance\n",
        "# Ward - minimizes the sum of squared differences within all clusters. It is a variance-minimizing approach and in this sense is similar to the k-means objective function but tackled with an agglomerative hierarchical approach.\n",
        "# Maximum or \"complete\" - minimizes the maximum distance between observations of pairs of clusters.\n",
        "# Average - minimizes the average of the distances between all observations of pairs of clusters.\n",
        "# Single - minimizes the distance between the closest observations of pairs of clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVoeYY5v8nU5"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "AHC = AgglomerativeClustering(n_clusters=7, affinity='euclidean', linkage='ward').fit(X)\n",
        "clusters = pd.DataFrame(AHC.labels_, columns=['cluster'])\n",
        "detail = pd.merge(left=cluster_CE_PE.reset_index(), right=clusters, left_index=True, right_index=True)\n",
        "detail[['PIB','DENS']]=detail[['PIB','DENS']].astype(float)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15,6))\n",
        "ax.set(xscale=\"log\", yscale=\"log\")\n",
        "\n",
        "sns.scatterplot(ax=ax, x='DENS', y='PIB', hue='cluster', data=detail)\n",
        "\n",
        "for x,y,t in zip(detail['DENS'], detail['PIB'], detail['CIDADE']):\n",
        "    if y > 450:\n",
        "        ax.text(x-10, y-1, t)\n",
        "\n",
        "ax.set_xlabel('Densidade (hab/km2)')\n",
        "ax.set_ylabel('PIB per Capita')\n",
        "#ax.set_xlim(10, 500)\n",
        "#ax.set_ylim(300, 1100)\n",
        "sns.despine(ax=ax)\n",
        "ax.legend(frameon=False)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz0Ox8gl8nU5"
      },
      "source": [
        "Isso parece funcionar muito bem também. Esses grupos parecem bastante naturais."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRcHQsRl8nU5"
      },
      "source": [
        "## Resumo de diferentes abordagens\n",
        "\n",
        "A figura abaixo é da documentação de clustering do scikit e demonstra como diferentes abordagens de clustering (algumas que eu não discuti) fazem para diferentes variações nos dados (fingidos). Os números no canto inferior direito apresentam o tempo de computação que pode ser um problema à medida que seus dados se tornam maiores e o algoritmo de agrupamento se torna mais complexo.\n",
        "\n",
        "<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png\">\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "5-1 Clustering.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}